{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 07: Ensemble Methods – Gradient Boosting\n",
    "\n",
    "## Due: Midnight on June 30 (with 2-hour grace period) and worth 50 points\n",
    "\n",
    "Over the past two weeks, we have expanded our machine learning toolkit by moving beyond linear regression to explore decision trees, which require finding optimal settings for a large set of interacting parameters. Through Homework 6, you developed a systematic workflow for parameter tuning that balances manual exploration and automated searches (e.g., grid search) to optimize performance while gaining insights into model behavior.\n",
    "\n",
    "This week, we take another step forward by studying ensemble methods which combine multiple decision trees to produce even stronger predictive models. Specifically, we will investigate **Gradient Boosting,** currently the SOTA among non-deep learning methods.   \n",
    "\n",
    "\n",
    "### What We Will Do in This Homework\n",
    "\n",
    "To analyze and optimize our ensemble models, we will apply the two-phase strategy introduced in Homework 6. However, we will **add two new tools** to our toolbox for tuning models:\n",
    "\n",
    "- We will employ **randomized grid search before exhaustive grid search** to efficiently explore the hyperparameter space and identify promising regions without the high computational cost of evaluating every possible combination.\n",
    "- You will **store your best parameter values (and the resulting CV MAE) in a dictionary** in order to track improvements across experiments and maintain a clear record of how each parameter choice was made.  This is essential when doing manual tuning. \n",
    "\n",
    "Our two-phase strategy is thus (with the new features in italics):\n",
    "1. **First Phase:**  \n",
    "   - Iteratively sweep through key parameters in **coarse ranges**, *keeping track of results in a dictionary*  \n",
    "   - Visualize training, validation, and test MAE  \n",
    "   - Diagnose overfitting or underfitting  \n",
    "   - Repeat the iterative sweep with a finer granularity for each parameter (for example, for `n_estimators` you might start with 100, 150,  etc. for the first set of sweeps, then try 125, 150,  ...)\n",
    "\n",
    "2. **Second Phase:**  \n",
    "   - Focus on the most unstable or promising parameter ranges found in Phase 1  \n",
    "   - *Perform a random search within these narrower ranges using `RandomizedSearch`* \n",
    "   - Perform an exhaustive grid search within an even narrower range using `GridSearchCV` with the finest appropriate granularity (e.g., for `n_estimators` it might be 800, 801, 802, ..., 849, 850)\n",
    "\n",
    "We will follow this process for Gradient Boosting Regressor, systematically tuning the most important parameters (see **Appendix 1** for a complete list):\n",
    " \n",
    "> `n_estimators`, `max_depth`, `max_features`, `min_samples_split`, `min_samples_leaf` \n",
    "\n",
    "**Note that we will NOT be tuning the learning rate.**  \n",
    "\n",
    ">Gradient Boosting Trees can appear to improve endlessly as you add more and more estimators (and decrease the learning rate correspondingly), however, there is a serious risk of **overfitting**.  There are various ways of controlling this, for example, using Early Stopping, or by keep track of the standard deviation of the CV scores, or by comparing with the test scores, but for the purposes of this homework, we will control overfitting in a naive way by keeping the learning rate fixed at its default value and tuning the other parameters. The point is to focus on the workflow of manual and automated parameter tuning. \n",
    "\n",
    "We will continue using `RepeatedKFold` cross-validation to reduce variance in our CV MAE estimates. The default number of repetitions is 5, but you may find it necessary to reduce this when initially searching broad parameter spaces—then increase it for fine-tuning as you zero-in on the best models. \n",
    "\n",
    "\n",
    "\n",
    "### Before Starting\n",
    "- Review lesson materials on ensemble methods and pay special attention to the Gradient Boosting video and notebook, as the code in this homework builds on (and has been adapted from) those resources.\n",
    "- Refer to **Appendix 2** for more complete advice about tuning strategies, and **Appendix 3** for an explanation of randomized grid search in Keras and a comparison with exhaustive search.  \n",
    "  \n",
    "\n",
    "### Grading\n",
    "\n",
    "This homework consists of 5 graded problems, each worth 10 points, for a total of 50 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "\n",
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble        import GradientBoostingRegressor\n",
    "from sklearn.metrics         import mean_absolute_error\n",
    "from tqdm                    import tqdm\n",
    "\n",
    "import matplotlib.ticker as mticker           # Optional: you can print out y axis labels as dollars. \n",
    "\n",
    "# globals\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "# utility code\n",
    "\n",
    "# Optional:  Format y-axis labels as dollars with commas\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Ames Housing Dataset  \n",
    "\n",
    "The code cell below will load the dataset for you.  This is the same dataset we used for the last two homeworks. \n",
    "\n",
    "> **Notice** that this code includes a useful optimization: **before downloading, it first\n",
    "checks whether the files already exist.** This is a essential step when working with large datasets or when building deep learning models, where training can span hours or even days. By reusing previously downloaded files or saved models, you can avoid unnecessary work and significantly speed up your workflow.\n",
    "\n",
    "For a detailed description of the dataset features, please refer to the **Appendix** in Homework 05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files already exist. Skipping download.\n",
      "Training and testing datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"Ames_Dataset\"                              # Directory where files will be stored\n",
    "\n",
    "# Check if one of the files exists; if not, download and extract the zip file\n",
    "\n",
    "if not os.path.exists( os.path.join(data_dir, \"X_train.csv\") ):\n",
    "    print(\"Dataset files not found. Downloading...\")\n",
    "    zip_url = \"https://www.cs.bu.edu/fac/snyder/cs505/Data/ames_housing.zip\"\n",
    "    try:\n",
    "        response = requests.get(zip_url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        # Extract the zip file into the designated directory\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "        print(\"Files downloaded and extracted successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "else:\n",
    "    print(\"Dataset files already exist. Skipping download.\")\n",
    "\n",
    "# Load the datasets\n",
    "X_train = pd.read_csv(os.path.join(data_dir, \"X_train.csv\"))\n",
    "X_test  = pd.read_csv(os.path.join(data_dir, \"X_test.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(data_dir, \"y_train.csv\")).squeeze(\"columns\")    \n",
    "y_test  = pd.read_csv(os.path.join(data_dir, \"y_test.csv\")).squeeze(\"columns\")\n",
    "\n",
    "print(\"Training and testing datasets loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude: Wrapper Functions for Running Ensemble Models\n",
    "\n",
    "The following cells are adapted from the Week 7 video notebook on `GradientBoostingRegressor`, but have been refactored to be more generally useful (perhaps in your final project):\n",
    "\n",
    "- **`run_model`** replaces the original `run_gradient_boosting_regressor` and accepts a parameter dictionary that can be applied to any model. You do not need to call this explicitly in this homework. \n",
    "- **`sweep_parameter`** is updated to work seamlessly with `run_model`, letting you:\n",
    "  - Specify which model you want to use;  \n",
    "  - Pass a dictionary of model parameters; and  \n",
    "  - Return a modified parameter dictionary reflecting the best value of the parameter you swept, along with the corresponding MAE.\n",
    "\n",
    "**Note:** Please do not change these cells unless you consult with the LFs first. Any alterations may cause downstream issues with the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model for testing and returning metrics.\n",
    "# NOTE: You can NOT use this for running the model on the test set.\n",
    "\n",
    "def run_model(model, \n",
    "              X_train, y_train, \n",
    "              n_repeats=5, \n",
    "              n_jobs=-1, \n",
    "              **model_params\n",
    "             ):\n",
    "\n",
    "    # Instantiate the model if a class is provided\n",
    "    if isinstance(model, type):\n",
    "        model = model(**model_params)\n",
    "    else:                                    \n",
    "        model.set_params(**model_params)    \n",
    "\n",
    "    # Use negative MAE for cross-validation (since sklearn minimizes loss)\n",
    "    neg_mae_scores = cross_val_score(\n",
    "        model, \n",
    "        X_train, y_train,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=RepeatedKFold(n_splits=5, n_repeats=n_repeats, random_state=random_seed), \n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    \n",
    "    mean_cv_mae = -np.mean(neg_mae_scores)\n",
    "    std_cv_mae  =  np.std(neg_mae_scores)\n",
    "    \n",
    "    # Fit the model on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute training MAE\n",
    "    train_preds = model.predict(X_train)\n",
    "    train_mae   = mean_absolute_error(y_train, train_preds)\n",
    "    \n",
    "    return mean_cv_mae, std_cv_mae, train_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_parameter(model,\n",
    "                    Parameters,\n",
    "                    param,\n",
    "                    parameter_list,\n",
    "                    X_train          = X_train,              # Defined above\n",
    "                    y_train          = y_train,\n",
    "                    verbose          = True,\n",
    "                    show_mae         = True,\n",
    "                    show_std         = False,\n",
    "                    n_iter_no_change = None,\n",
    "                    delta            = 0.001,\n",
    "                    n_jobs           = -1,\n",
    "                    n_repeats        = 5\n",
    "                   ):\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "    Parameters = Parameters.copy()  # Avoid modifying the original dictionary\n",
    "    \n",
    "    cv_maes, std_cvs, train_maes = [], [], []\n",
    "    no_improve_count = 0\n",
    "    best_mae = float('inf')\n",
    "    \n",
    "    # Run over each value in parameter_list\n",
    "    for p in tqdm(parameter_list, desc=f\"Sweeping {param}\"):\n",
    "        Parameters[param] = p\n",
    "        P_temp = Parameters.copy()\n",
    "        P_temp.pop('MAE_found', None)  # Just in case\n",
    "        \n",
    "        cv_mae, std_cv, train_mae = run_model(\n",
    "            model=model,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            n_repeats=n_repeats,\n",
    "            n_jobs=n_jobs,\n",
    "            **P_temp\n",
    "        )\n",
    "        cv_maes.append(cv_mae)\n",
    "        std_cvs.append(std_cv)\n",
    "        train_maes.append(train_mae)\n",
    "        \n",
    "        if cv_mae < best_mae - delta:\n",
    "            best_mae = cv_mae\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "        \n",
    "        if n_iter_no_change is not None and no_improve_count >= n_iter_no_change:\n",
    "            print(f\"Early stopping: No improvement after {n_iter_no_change} iterations.\")\n",
    "            break\n",
    "\n",
    "    # Identify best parameter\n",
    "    min_cv_mae = min(cv_maes)\n",
    "    min_index = cv_maes.index(min_cv_mae)\n",
    "    best_param = parameter_list[min_index]\n",
    "    Parameters[param] = best_param\n",
    "    Parameters['MAE_found'] = min_cv_mae\n",
    "\n",
    "    # ---------- Plotting section ----------\n",
    "    if verbose:\n",
    "        partial_param_list = parameter_list[:len(cv_maes)]\n",
    "\n",
    "        is_boolean = all(isinstance(val, bool) for val in partial_param_list)\n",
    "        if is_boolean:\n",
    "            x_vals = list(range(len(partial_param_list)))\n",
    "            x_labels = [str(val) for val in partial_param_list]\n",
    "        else:\n",
    "            x_vals = partial_param_list\n",
    "            x_labels = partial_param_list\n",
    "\n",
    "        error_name = 'MAE'\n",
    "\n",
    "        # Create appropriate number of subplots\n",
    "        if show_std:\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
    "        else:\n",
    "            fig, ax1 = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "        ax1.set_title(f\"{error_name} vs {param}\")\n",
    "        if show_mae:\n",
    "            ax1.yaxis.set_major_formatter(mticker.FuncFormatter(dollar_format))\n",
    "\n",
    "        ax1.plot(x_vals,\n",
    "                 cv_maes,\n",
    "                 marker='.', label=\"CV MAE\", color='blue')\n",
    "        ax1.plot(x_vals,\n",
    "                 train_maes,\n",
    "                 marker='.', label=\"Train MAE\", color='green')\n",
    "        ax1.scatter([x_vals[min_index]],\n",
    "                    [min_cv_mae],\n",
    "                    marker='x', label=\"Best CV MAE\", color='red')\n",
    "\n",
    "        ax1.set_ylabel(error_name)\n",
    "        ax1.legend()\n",
    "        ax1.grid()\n",
    "\n",
    "        # Optional Std Dev Plot\n",
    "        if show_std:\n",
    "            ax2.set_title(f\"CV Standard Deviation vs {param}\")\n",
    "            ax2.plot(x_vals, std_cvs, marker='.', label=\"CV MAE Std\", color='blue')\n",
    "            ax2.set_xlabel(param)\n",
    "            ax2.set_ylabel(\"Standard Deviation\")\n",
    "            ax2.legend()\n",
    "            ax2.grid(alpha=0.5)\n",
    "\n",
    "            if is_boolean:\n",
    "                ax2.set_xticks(x_vals)\n",
    "                ax2.set_xticklabels(x_labels)\n",
    "        else:\n",
    "            ax1.set_xlabel(param)\n",
    "            if is_boolean:\n",
    "                ax1.set_xticks(x_vals)\n",
    "                ax1.set_xticklabels(x_labels)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"Execution Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end - start)))\n",
    "\n",
    "    return Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Iterative Parameter Sweeping and Visualization with `sweep_parameter(...)`\n",
    "\n",
    "In this problem, you’ll tune six key hyperparameters of `GradientBoostingRegressor` by manually sweeping their values and visualizing the results using **Mean Absolute Error (MAE)** as your evaluation metric. \n",
    "\n",
    "\n",
    "We’ll accelerate the tuning workflow introduced in Homework 6 by updating our estimate of the optimal parameter value after **each** sweep of an individual parameter.\n",
    "\n",
    "For each sweep, `sweep_parameter(...)` will allow you to:\n",
    "\n",
    "* **Test a range of values** for the selected parameter\n",
    "* **Plot**:\n",
    "\n",
    "  * Training MAE\n",
    "  * Repeated cross-validation MAE\n",
    "\n",
    "  These plots will help you diagnose **overfitting** or **underfitting**\n",
    "\n",
    "* **Record the best value**—the one that yields the lowest mean CV MAE—in `Parameters_GB`\n",
    "\n",
    "You’ll invoke it like this:\n",
    "\n",
    "```python\n",
    "Params_GB = sweep_parameter(GradientBoostingRegressor,   \n",
    "                            Params_GB,   \n",
    "                            ...\n",
    "```\n",
    "Begin by copying the provided `Default_Parameters_GB` dictionary to a new dictionary called `Parameters_GB`.\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "Your objective is to apply a **coarse-to-fine tuning strategy**: begin with broad parameter ranges and coarse steps, then narrow the range and increase granularity as you approach a good model. This prepares you for later stages where **randomized** or **grid search** can be used to fine-tune.\n",
    "\n",
    "For example, when sweeping `n_estimators`, you might proceed as follows:\n",
    "\n",
    "* Start with `range(100, 1001, 100)`\n",
    "* Narrow to `range(800, 1001, 50)`\n",
    "* Refine further with `range(900, 1001, 25)` etc.\n",
    "\n",
    "### Step-by-Step Sweeping Procedure\n",
    "\n",
    "**Tip:** Use 5 repeats for `RepeatedKFold` scoring for stable CV estimates. For broad sweeps, you may reduce to 1–2 repeats to speed up experimentation, but be sure to restore to 5 before finalizing.\n",
    "\n",
    "\n",
    "\n",
    "1. **Sweep `n_estimators`** (*integer values*):\n",
    "\n",
    "   * Test a range of values (e.g., 200 ..., 1000)\n",
    "   * Begin with a step size of 100\n",
    "   * Plot results and update `Parameters_GB` with the value yielding the lowest mean CV MAE\n",
    "\n",
    "2. **Sweep `max_depth`** (*integer values*):\n",
    "\n",
    "   * Test tree depths such as 5, 10, ...\n",
    "   * Use a step size of 5 initially\n",
    "   * Plot results and store the best value in `Parameters_GB`\n",
    "\n",
    "3. **Sweep `max_features`** (*integer values*):\n",
    "\n",
    "   * Try integers such as 5, 10, 15...  ('sqrt' and 'log2' are possible, but integers are more precise)\n",
    "   * Start with a step size of 5\n",
    "   * Check `X_train.shape` to see the maximum number of features\n",
    "   * Plot results and store the best value in `Parameters_GB`\n",
    "\n",
    "4. **Sweep '`min_samples_split`** (*integer values*):\n",
    "\n",
    "   * Test integers such as 2, 7, 13 ...\n",
    "   * Use a step size of 5 initially\n",
    "   * Plot results and store the best value in `Parameters_GB`\n",
    "\n",
    "5. **Sweep `min_samples_leaf`** (*integer values*):\n",
    "\n",
    "   * Try integers such as 1, 6, 11 ...  \n",
    "   * Start with a step size of 5\n",
    "   * Plot results and store the best value in `Parameters_GB`\n",
    "\n",
    "6. **Repeat with Finer Granularity**\n",
    "   After your first pass, repeat steps 1–5.  I generally repeat it once without changing the range, and then I try it with narrower ranges and smaller step sizes. You don't have to change each parameter each time through. Eventually, you want to end up with fairly narrow ranges with medium-sized steps:\n",
    "\n",
    "   * `n_estimators`: step size of **10** or even **5**\n",
    "   * For the others, a step size of **2** or even **1**\n",
    "\n",
    "   Your goal is to home in on the optimal parameter region, using CV MAE  as guide.\n",
    "\n",
    "\n",
    "\n",
    "6. **Ensure Robustness**\n",
    "\n",
    "   * Your final sweep should use **at least 5 repeats** for `RepeatedKFold`\n",
    "   * This final configuration will form the foundation for Problem 2\n",
    "\n",
    "\n",
    "### Final Reporting\n",
    "\n",
    "After completing 2 - 5 rounds of parameter sweeping:\n",
    "\n",
    "* Report the final tuned values stored in `Parameters_GB`\n",
    "* Display the final **MAE** clearly, in **dollars**\n",
    "* Respond to the associated **graded question**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Default_Parameters_GB = {\n",
    "    'n_estimators'            : 100,             # The number of boosting stages to be run. More estimators can improve performance but increase training time.\n",
    "    'max_depth'               : 3,               # Maximum depth of individual trees. Controls model complexity.\n",
    "    'max_features'            : None,            # Number of features to consider when looking for best split. Can help reduce overfitting.\n",
    "    'min_samples_split'       : 2,               # Defines the minimum number of samples required to split an internal node.\n",
    "    'min_samples_leaf'        : 1,               # Specifies the minimum number of samples that must be present in a leaf node. \n",
    "    'random_state'            : random_seed,     # Controls randomness of boosting. Useful for reproducibility.\n",
    "    'MAE_found'               : float('inf')     # NOT a model parameter, but will record the MAE found for the current parameter choices\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the default dictionary\n",
    "\n",
    "Params_GB = Default_Parameters_GB.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, add as many cells as you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 Graded Answer\n",
    "\n",
    "Set `a1` to the CV MAE score of your best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Your answer here\n",
    "\n",
    "a1 = 0                     # replace 0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 = $0.00\n"
     ]
    }
   ],
   "source": [
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a1 = ${a1:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Use `RandomizedSearch` to explore alternatives \n",
    "Once you have completed your parameter sweeps in Problem 2, you will explore the ranges you used in Problem 1 (or even narrower ranges) using randomized grid search (not grid search yet).  \n",
    "\n",
    "Use the final ranges you used in Problem 1, or restrict them, but:\n",
    "- Use `randint(lb,ub)` for all parameters (it will use to a granularity of 1)\n",
    "- Do as many repetitions as you can without running it for hours. Try for at least 100.\n",
    "- Be sure to use repeated CV scoring to be consistent with Problem 1.\n",
    "- Print out the best result found by randomized search (including the final parameter choices) and answer the graded question.\n",
    "\n",
    "**Note:** You may not get a better result than in Problem 1, but it is always worth trying randomized search!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 Graded Answer\n",
    "\n",
    "Set `a2` to the best CV MAE score found using randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Your answer here\n",
    "\n",
    "a2 = 0                     # replace 0 with your answer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2 = $0.00\n"
     ]
    }
   ],
   "source": [
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a2 = ${a2:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Confirmation: Use `GridSearchCV` for Exhaustive Search  \n",
    "\n",
    "Now you have a pretty good idea of what the best parameter ranges are, and you have probed with randomized search to investigate the search space further. \n",
    "- Perform  **exhaustive grid searches** within appropriately restricted ranges using `GridSearchCV`. If possible, they should not be the same as you used for Problem 1, but even further restricted (so that it does not run for hours!). Use a granularity of 1 for all the parameters.\n",
    "- **Print out the best result found by exhaustive search (including the final parameter choices** and answer the graded question.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 Graded Answer\n",
    "\n",
    "Set `a3` to the best CV MAE score found using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Your answer here\n",
    "\n",
    "a3 = 0                     # replace 0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3 = $0.00\n"
     ]
    }
   ],
   "source": [
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a3 = ${a3:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Evaluate Your Models\n",
    "At this point, you *may* have **three different models**:  \n",
    "- The model found in **Problem 1** (from parameter sweeps).  \n",
    "- The model found in **Problem 2** (from `RandomizedSearch`).\n",
    "- The model found in **Problem 3** (from `GridSearchCV`).   \n",
    "\n",
    "If you have done this correctly, grid search *should* have either confirmed your best model from Problem 1 or 2, or\n",
    "found an even better model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 Graded Answer\n",
    "\n",
    "Set `a4` to the number of the best model found:\n",
    "- 1 = Problem 1 model, confirmed by Problem 3 model\n",
    "- 2 = Problem 2 model, better than Problem 1, and confirmed by Problem 3\n",
    "- 3 = Problem 3 model, better than Problem 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Your answer here\n",
    "\n",
    "a4 = 0                     # replace 0 with one of 1, 2, or 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a4 = 0\n"
     ]
    }
   ],
   "source": [
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a4 = {a4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Report the Test Score of the Best Model  \n",
    "Once you have selected the best model in Problem 4, report its **final test score** and answer the graded question.  \n",
    "\n",
    "**Note:** You can not use `run_model` for this, as it is set up only for training runs. Create a gradient boosting model using the optimal parameters, fit it to the training set, and then generate predictions from the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 Graded Answer\n",
    "\n",
    "Set `a5` to the test MAE of the model you selected in Problem 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Your answer here\n",
    "\n",
    "a5 = 0                     # replace 0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a5 = $0.00\n"
     ]
    }
   ],
   "source": [
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a5 = ${a5:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1: Which `GradientBoostingRegressor` parameters are most important?\n",
    "\n",
    "We will focus on the top **four** parameters in this list for `GradientBoostingRegressor`. \n",
    "\n",
    "---\n",
    "\n",
    "**Most Important Parameters**\n",
    "\n",
    "1. **learning_rate** (default: **0.1**)  \n",
    "   *Controls the contribution of each individual tree. A lower learning rate generally requires more trees but can lead to improved generalization.*\n",
    "\n",
    "2. **n_estimators** (default: **100**)  \n",
    "   *Specifies the number of boosting stages (i.e., the number of trees in the ensemble). More estimators can improve performance but also increase the risk of overfitting if not tuned properly.*\n",
    "\n",
    "3. **max_depth** (default: **3**)  \n",
    "   *Limits the depth of the individual regression trees. Restricting the depth helps control overfitting and reduces the complexity of each base learner.*\n",
    "\n",
    "4. **max_features** (default: **None**)  \n",
    "   *Controls the number of features to consider when looking for the best split. Adjusting this can influence the bias-variance trade-off of the model.*\n",
    "\n",
    "5. **min_samples_split** (default: **2**)  \n",
    "   *Defines the minimum number of samples required to split an internal node. This parameter controls the growth of each tree and can prevent overly specific splits.*\n",
    "\n",
    "6. **min_samples_leaf** (default: **1**)  \n",
    "   *Specifies the minimum number of samples that must be present in a leaf node. This helps in ensuring that trees do not become too tailored to the training data.*\n",
    "\n",
    "---\n",
    "\n",
    "**Less Important Parameters**\n",
    "\n",
    "7. **max_leaf_nodes** (default: **None**)  \n",
    "    *An optional parameter that sets a maximum number of leaf nodes for each tree. This can provide an additional way to control the complexity of the model.*\n",
    "\n",
    "8. **subsample** (default: **1.0**)  \n",
    "   *Determines the fraction of samples used for fitting each individual tree. Values less than 1.0 introduce randomness into the boosting process, which can help reduce overfitting.*\n",
    "\n",
    "9. **loss** (default: **'squared_error'**)  \n",
    "   *Determines the loss function to be optimized during training. Different loss functions can be used depending on the specific characteristics of the regression problem.*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Appendix 2: Tuning and Selecting Complex Models\n",
    "\n",
    "This appendix offers practical guidance for tuning complex models like Gradient Boosting and for interpreting validation results to choose the best-performing configuration. It combines strategy, visualization, and decision-making heuristics into one workflow.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Using `sweep_parameters` for Single-Parameter Exploration\n",
    "\n",
    "The function `sweep_parameters` automates parameter tuning by iterating over a range of values (e.g., `n_estimators`) and tracking performance across:\n",
    "\n",
    "* **Training MAE**: Fit to training data\n",
    "* **Cross-Validation (CV) MAE**: Generalization estimate across folds (and repeats)\n",
    "* **Test MAE**: Out-of-sample check on a held-out test set\n",
    "\n",
    "**How to Interpret the Plots:**\n",
    "\n",
    "* **Training vs. CV MAE**: A growing gap often signals overfitting; high values for both may indicate underfitting.\n",
    "* **CV MAE Curve**: Choose values near the minimum (valley), ideally where the curve flattens.\n",
    "* **Watch Plot Scales**: A flat-looking curve may conceal meaningful differences if the y-axis scale is tight. Always consider actual values, not just shapes.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Tuning Strategy: Coarse-to-Fine\n",
    "\n",
    "* Start with **broad ranges and coarse steps** (e.g., 100–1000 by 100).\n",
    "* Once you find a promising region, **narrow the range and reduce step size** (e.g., 500–1000 by 25 or 10).\n",
    "\n",
    "\n",
    "### 3. Using Repeated Cross-Validation Effectively\n",
    "\n",
    "**Why Repeat?**\n",
    "Repeated CV provides more stable estimates by averaging results across multiple random folds, reducing variance due to data splits.\n",
    "\n",
    "**How Many Repeats?**\n",
    "\n",
    "* **Early (Broad Sweeps):** 1–2 repeats for speed\n",
    "* **Fine Tuning:** 5–10 repeats for stability and confidence in final selection\n",
    "\n",
    "**Trade-Offs:**\n",
    "\n",
    "* More repeats increase reliability, but also computation time. Scale up only after narrowing your search.\n",
    "\n",
    "\n",
    "### 4. Model Selection: Key Indicators\n",
    "\n",
    "When comparing models or parameter settings:\n",
    "\n",
    "* **Minimize Mean CV MAE**: This is your primary signal of generalization performance during model tuning.\n",
    "\n",
    "* **Look for Stability**: Favor flatter regions (plateaus) near the minimum of the CV MAE curve, rather than sharp dips that may reflect overfitting or noise.\n",
    "\n",
    "* **Avoid Overfitting**: If you examine the test MAE **after tuning,** be cautious of a *growing gap* between CV MAE and test MAE.\n",
    "\n",
    "  * This can suggest the model is fitting cross-validation folds too tightly.\n",
    "  * However, do **not** use test MAE to guide parameter choices — reserve it as a **final check** only.\n",
    "\n",
    "* **[Optional] Prioritize Consistency**: A lower standard deviation of CV scores indicates more stable performance across different splits.\n",
    "\n",
    "* **Interpret Plots Carefully**: Always consider the **scale** of the y-axis when comparing curves. Flat-looking trends might conceal meaningful differences if the axis range is small.\n",
    "\n",
    "\n",
    "### 5. Workflow Tips for Efficient Tuning\n",
    "\n",
    "* **Visualize Everything**: Always plot training/CV/test MAE of CV scores for insight.\n",
    "* **Track Experiments**: Use a dictionary (or a list of dictionaries) to record parameter settings and results.\n",
    "* **Scale Up Thoughtfully**: Start simple and add complexity (more repeats, finer search) only when needed.\n",
    "* **Use `GridSearchCV` and `RandomizedSearchCV` Judiciously**:\n",
    "\n",
    "  * Start with random search to identify promising regions, then use grid search for final tuning.\n",
    "  * Be aware that both methods may take significant time—especially without a progress bar.\n",
    "* **Enable Parallelism with `n_jobs=-1`**: This will use all available CPU cores.\n",
    "\n",
    "  * If you see warnings (e.g., “a worker stopped”), you may be running out of memory. Reduce `n_jobs` (e.g., to 4) if needed.\n",
    "\n",
    "\n",
    "\n",
    "By combining thoughtful parameter sweeps, smart use of repeated CV, and careful reading of validation curves and variance, you’ll build models that not only perform well but generalize reliably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 3: Randomized Search vs. Grid Search for Gradient Boosting\n",
    "\n",
    "This appendix compares two strategies for hyperparameter tuning—**randomized search** and **grid search**—using `GradientBoostingRegressor` as the working example. \n",
    "\n",
    "### 1. Problem Setup\n",
    "\n",
    "Suppose you are training a `GradientBoostingRegressor` and want to optimize its predictive performance by tuning the key hyperparameters above.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=random_seed)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2. Grid Search\n",
    "\n",
    "**Grid search** evaluates **all** combinations of parameter values in a predefined grid.\n",
    "\n",
    "#### Define Parameter Grid\n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "\n",
    "    'n_estimators'      : range(1000,1501,100),\n",
    "    'max_depth'         : range(5,51,5),\n",
    "    'max_features'      : [3, 4, 5],\n",
    "    'min_samples_split' : [2,4,6,8],             \n",
    "    'min_samples_leaf'  : [1,2,3],   \n",
    "}\n",
    "```\n",
    "\n",
    "#### Run `GridSearchCV`\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=5, random_state=42), \n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "```\n",
    "\n",
    "* **Pros**: Exhaustive—guarantees best settings *within* your grid\n",
    "* **Cons**: Combinatorial explosion—3×3×3×2 = 54 fits! Time and resources grow rapidly\n",
    "\n",
    "\n",
    "\n",
    "### 3. Randomized Search\n",
    "\n",
    "**Randomized search** samples a fixed number of combinations from **distributions** over the parameter space.\n",
    "\n",
    "#### Define Distributions\n",
    "\n",
    "This is a significant difference with grid search: you must specify a random-number generator instead of\n",
    "giving a list of explicit values. \n",
    "\n",
    "```python\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators'      : randint(100, 500),        # integers from 100 to 499\n",
    "    'max_depth'         : randint(3, 8),            # integers from 3 to 7\n",
    "    'max_features'      : randint(2, 15),            \n",
    "    'min_samples_split' : randint(2, 10),             \n",
    "    'min_samples_leaf'  : randint(1, 8),             \n",
    "}\n",
    "```\n",
    "\n",
    "#### Run `RandomizedSearchCV`\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rand = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,                 # try 20 random combinations\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_seed), \n",
    "    scoring='neg_mean_absolute_error',\n",
    "    random_state=random_seed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rand.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", rand.best_params_)\n",
    "```\n",
    "\n",
    "* **Pros**: Efficient—fixed number of combinations regardless of parameter count\n",
    "* **Cons**: May miss the absolute best—but often finds near-optimal solutions much faster\n",
    "\n",
    "\n",
    "\n",
    "### 4. When to Use Which?\n",
    "\n",
    "| Scenario          | Use Grid Search                 | Use Randomized Search               |\n",
    "| ----------------- | ------------------------------- | ----------------------------------- |\n",
    "| Search space size | Small, well-defined             | Large or continuous                 |\n",
    "| Compute budget    | High                            | Moderate or limited                 |\n",
    "| Goal              | Exhaustive search within a grid | Fast discovery of promising regions |\n",
    "| Parameter types   | Discrete                        | Continuous or mixed                 |\n",
    "| Tuning stage      | Final fine-tuning               | Early-stage exploration             |\n",
    "\n",
    "\n",
    "\n",
    "### 5. Best Practices\n",
    "\n",
    "1. **Start with Randomized Search**\n",
    "\n",
    "   * Identify promising parameter ranges quickly and cheaply.\n",
    "\n",
    "2. **Refine with Grid Search**\n",
    "\n",
    "   * Use a tighter grid centered around the best values from the randomized search.\n",
    "\n",
    "3. **Use Log Scale for Learning Rates**\n",
    "\n",
    "   * For scale-sensitive values like `learning_rate`, sample from log-distributions (e.g., `loguniform(1e-3, 1e0)`).\n",
    "\n",
    "4. **Validate on a Held-Out Test Set**\n",
    "\n",
    "   * After tuning, always check your model’s performance on a separate test set to assess generalization.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
